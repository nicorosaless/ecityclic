{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the original CSV file\n",
    "df = pd.read_csv('../../data/accions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Sessio' and aggregate the 'Accio' column into a list\n",
    "grouped_df = df.groupby('Sessio').agg({'Accio': list, 'Tramit': list, 'Usuari': list, 'Data': list}).reset_index()\n",
    "\n",
    "\n",
    "# Write the grouped data to a new CSV file\n",
    "grouped_df.to_csv('../../data/grouped_accions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df['Usuari'] = grouped_df['Usuari'].apply(lambda x: x[-1] if isinstance(x, list) and x else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the grouped data to a new CSV file\n",
    "grouped_df.to_csv('../../data/grouped_accions_usuari.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped_df = pd.read_csv('../../data/grouped_accions_usuari.csv')\n",
    "(grouped_df['Tramit'].apply(lambda x: len((x))) == 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = grouped_df[~((grouped_df['Usuari'].isna()) & (grouped_df['Tramit'].apply(lambda x: len((x))) == 1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the grouped data to a new CSV file\n",
    "filtered_df.to_csv('../../data/grouped_accions_filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = pd.read_csv('../../data/grouped_accions_filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = filtered_df[~(filtered_df['Tramit'].apply(lambda x: len(eval(x))) == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.drop(columns=['Usuari', 'Accio', 'Data', 'Sessio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('../../data/grouped_accions_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.read_csv('../../data/grouped_accions_final.csv')\n",
    "\n",
    "#### \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['Last_Tramit'] = final_df['Tramit'].apply(lambda x: eval(x)[-1] if isinstance(eval(x), list) and eval(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['Tramit'] = final_df['Tramit'].apply(lambda x: str(eval(x)[:-1]) if isinstance(eval(x), list) and eval(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('../../data/final_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final_df = pd.read_csv('../../data/final_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_final_df['Tramit']\n",
    "y_train = train_final_df['Last_Tramit']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un dataset de muestra con 50 filas\n",
    "sample_df = train_final_df.sample(n=50, random_state=42)\n",
    "\n",
    "# Guardar el dataset de muestra en el directorio\n",
    "sample_df.to_csv('../../data/sample_train_final_df.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the sample_tramits.csv file\n",
    "sample_tramits_df = pd.read_csv('../../data/sample_tramits.csv')\n",
    "\n",
    "# Erase the 'Vigent' column\n",
    "sample_tramits_df = sample_tramits_df.drop(columns=['Vigent'])\n",
    "\n",
    "# Display the first few rows of the dataframe to verify\n",
    "print(sample_tramits_df.head())\n",
    "\n",
    "sample_tramits_df.to_csv('../../data/sample_tramits_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tramits_df = pd.read_csv('../../data/tramits.csv')\n",
    "\n",
    "\n",
    "tramits_df['Sequence'] = range(tramits_df.shape[0])\n",
    "\n",
    "\n",
    "tramits_df.to_csv('../../data/tramits.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_sequence = dict(zip(tramits_df['Id'], tramits_df['Sequence']))\n",
    "\n",
    "final_df['Tramit'] = final_df['Tramit'].apply(lambda x: [id_to_sequence.get(tramit, tramit) for tramit in eval(x)])\n",
    "final_df['Last_Tramit'] = final_df['Last_Tramit'].apply(lambda x: id_to_sequence.get(x, x))\n",
    "\n",
    "final_df.to_csv('../../data/final_dataset_mapped.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_df = pd.read_csv('../../data/final_dataset_mapped.csv')\n",
    "final_df = final_df[final_df['Tramit'].apply(len) <= 30]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Distribuci贸n de y_train:\", pd.Series(y_train).value_counts())\n",
    "\n",
    "print(\"Distribuci贸n del 82 en y_train:\", (y_train == 314).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, LayerNormalization, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "# ------------------ Carga y Preprocesamiento del Dataset ------------------\n",
    "print(\"### Carga y Preprocesamiento del Dataset ###\")\n",
    "file_path = \"../../data/final_dataset_mapped.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Preprocesar las columnas\n",
    "data['Tramit'] = data['Tramit'].apply(lambda x: ast.literal_eval(x))  # Convertir strings en listas\n",
    "if data.isnull().values.any():\n",
    "    raise ValueError(\"El archivo CSV contiene valores nulos.\")\n",
    "data['Last_Tramit'] = data['Last_Tramit'].astype(int)\n",
    "\n",
    "# Preparar las features 'X' y target 'y'\n",
    "max_seq_len = 20\n",
    "X_raw = data['Tramit'].tolist()\n",
    "y_data = data['Last_Tramit'].values\n",
    "X_data = pad_sequences(X_raw, maxlen=max_seq_len, padding='post', truncating='post')  # Padding de secuencias\n",
    "\n",
    "# Dividir los datos en entrenamiento (80%) y prueba (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# ------------------ Definici贸n de Capas Personalizadas ------------------\n",
    "print(\"### Definici贸n de Capas Personalizadas ###\")\n",
    "class TokenAndPositionEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim, **kwargs):\n",
    "        super(TokenAndPositionEmbedding, self).__init__(**kwargs)\n",
    "        print(f\"Inicializando TokenAndPositionEmbedding con maxlen={maxlen}, vocab_size={vocab_size}, embed_dim={embed_dim}\")\n",
    "        self.maxlen = maxlen\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Capas Embedding\n",
    "        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        max_len_positions = tf.range(start=0, limit=self.maxlen, delta=1)\n",
    "        positions = self.pos_emb(max_len_positions)  # Generar embeddings posicionales\n",
    "        tokens = self.token_emb(x)  # Generar embeddings de tokens\n",
    "        return tokens + positions  # Combinar tokens y posiciones\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(TokenAndPositionEmbedding, self).get_config()\n",
    "        config.update({\n",
    "            \"maxlen\": self.maxlen,\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "            \"embed_dim\": self.embed_dim\n",
    "        })\n",
    "        print(f\"Serializando capa TokenAndPositionEmbedding con config: {config}\")\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        print(f\"Reconstruyendo capa TokenAndPositionEmbedding desde config: {config}\")\n",
    "        return cls(**config)\n",
    "\n",
    "\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        print(f\"Inicializando TransformerBlock con embed_dim={embed_dim}, num_heads={num_heads}, ff_dim={ff_dim}, rate={rate}\")\n",
    "\n",
    "        # Guardar configuraciones\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.rate = rate\n",
    "\n",
    "        # Creaci贸n de capas\n",
    "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation=\"relu\"),\n",
    "            Dense(embed_dim)\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(TransformerBlock, self).get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"ff_dim\": self.ff_dim,\n",
    "            \"rate\": self.rate\n",
    "        })\n",
    "        print(f\"Serializando capa TransformerBlock con config: {config}\")\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        print(f\"Reconstruyendo capa TransformerBlock desde config: {config}\")\n",
    "        return cls(**config)\n",
    "\n",
    "# Resto del c贸digo se mantiene igual.\n",
    "# El modelo utilizar谩 ahora correctamente la clase TokenAndPositionEmbedding.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------ Definici贸n del Modelo ------------------\n",
    "\n",
    "print(\"### Definici贸n del Modelo ###\")\n",
    "\n",
    "def create_model(vocab_size, max_seq_len, embed_dim, num_heads, ff_dim, num_transformer_blocks, dropout_rate):\n",
    "    inputs = Input(shape=(max_seq_len,))\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen=max_seq_len, vocab_size=vocab_size, embed_dim=embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "\n",
    "    for i in range(num_transformer_blocks):\n",
    "        x = TransformerBlock(embed_dim, num_heads, ff_dim, rate=dropout_rate)(x)\n",
    "        print(f\"Creado TransformerBlock {i + 1} con Dropout Rate = {dropout_rate}\")\n",
    "\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    outputs = Dense(vocab_size, activation=\"softmax\")(x)\n",
    "    return Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "# ------------------ Ejecuci贸n Multi-Configuraci贸n ------------------\n",
    "print(\"### Ejecuci贸n Multi-Configuraci贸n ###\")\n",
    "configs = pd.DataFrame([  # ~20 min\n",
    "    # Modelos grandes:,# ~30 min\n",
    "    {\"embed_dim\": 512, \"num_heads\": 16, \"ff_dim\": 1024, \"num_transformer_blocks\": 4, \"epochs\": 8},# ~25-30 min\n",
    "    {\"embed_dim\": 256, \"num_heads\": 8, \"ff_dim\": 512, \"num_transformer_blocks\": 3, \"epochs\": 15}, # ~30+ min\n",
    "    {\"embed_dim\": 512, \"num_heads\": 16, \"ff_dim\": 1024, \"num_transformer_blocks\": 4, \"epochs\": 15},\n",
    "])\n",
    "\n",
    "if not os.path.exists('../../models/'):\n",
    "    os.makedirs('../../models/')\n",
    "\n",
    "results = []\n",
    "vocab_size = 502\n",
    "\n",
    "for index, config in configs.iterrows():\n",
    "    print(f\"\\n### Ejecutando Configuraci贸n {index + 1}/{len(configs)}: {config.to_dict()}\")\n",
    "    embed_dim = config['embed_dim']\n",
    "    num_heads = config['num_heads']\n",
    "    ff_dim = config['ff_dim']\n",
    "    num_transformer_blocks = config['num_transformer_blocks']\n",
    "    epochs = config['epochs']\n",
    "    dropout_rate = 0.2\n",
    "\n",
    "    # Crear el modelo\n",
    "    model = create_model(vocab_size, max_seq_len, embed_dim, num_heads, ff_dim, num_transformer_blocks, dropout_rate)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    start_time = time.time()\n",
    "    early_stop = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=32,\n",
    "        epochs=epochs,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stop],\n",
    "        verbose=1\n",
    "    )\n",
    "    training_time = time.time() - start_time\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "    # Guardar el modelo\n",
    "    model_save_path = f\"../../models/model_config_{index + 1}.h5\"\n",
    "    try:\n",
    "        print(f\"Guardando modelo en: {model_save_path}\")\n",
    "        model.save(model_save_path)\n",
    "        print(f\"Modelo guardado con 茅xito en {model_save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al guardar el modelo: {e}\")\n",
    "        continue  # Saltar esta configuraci贸n si falla\n",
    "\n",
    "    # Guardar resultados\n",
    "    results.append({\n",
    "        \"config_index\": index,\n",
    "        \"embed_dim\": embed_dim,\n",
    "        \"num_heads\": num_heads,\n",
    "        \"ff_dim\": ff_dim,\n",
    "        \"num_transformer_blocks\": num_transformer_blocks,\n",
    "        \"epochs\": epochs,\n",
    "        \"train_accuracy\": history.history['accuracy'][-1],\n",
    "        \"val_accuracy\": max(history.history['val_accuracy']),\n",
    "        \"test_accuracy\": test_accuracy,\n",
    "        \"test_loss\": test_loss,\n",
    "        \"training_time\": training_time,\n",
    "        \"model_path\": model_save_path\n",
    "    })\n",
    "\n",
    "# Consolidar resultados\n",
    "if len(results) > 0:  # Comprobar que haya resultados\n",
    "    try:\n",
    "        # Convertir lista de diccionarios en DataFrame\n",
    "        results_df = pd.DataFrame(results)\n",
    "\n",
    "        # Verificar que el DataFrame no est茅 vac铆o\n",
    "        if results_df.empty:\n",
    "            print(\"*** El DataFrame generado a partir de 'results' est谩 vac铆o. No se guardar谩 ning煤n archivo.\")\n",
    "        else:\n",
    "            # Guardar resultados en un archivo CSV\n",
    "            results_csv_path = '../../data/results.csv'\n",
    "\n",
    "            # Crear el directorio si no existe\n",
    "            output_dir = os.path.dirname(results_csv_path)\n",
    "            if not os.path.exists(output_dir):  # Verificar existencia de la carpeta\n",
    "                os.makedirs(output_dir)\n",
    "                print(f\"### Directorio creado: {output_dir}\")\n",
    "\n",
    "            # Guardar DataFrame en archivo CSV\n",
    "            results_df.to_csv(results_csv_path, index=False)\n",
    "            print(f\"### Resultados guardados en el archivo: {results_csv_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"*** Error al guardar los resultados: {str(e)}\")\n",
    "else:\n",
    "    print(\"*** No hay resultados para guardar. La lista 'results' est谩 vac铆a.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pruebas de predicciones\n",
    "\n",
    "# ------------------ Capas Personalizadas ------------------\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([tf.keras.layers.Dense(ff_dim, activation=\"relu\"), tf.keras.layers.Dense(embed_dim)])\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.dropout1(self.att(inputs, inputs, training=training), training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.dropout2(self.ffn(out1, training=training), training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "\n",
    "class TokenAndPositionEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = tf.keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        positions = self.pos_emb(tf.range(start=0, limit=tf.shape(x)[-1]))\n",
    "        return self.token_emb(x) + positions\n",
    "\n",
    "\n",
    "# ------------------ Modelo Preentrenado ------------------\n",
    "def cargar_modelo(ruta_modelo=\"modelo_transformer_tramites.h5\"):\n",
    "    \"\"\"Carga el modelo Transformer preentrenado.\"\"\"\n",
    "    model = tf.keras.models.load_model(\n",
    "        ruta_modelo,\n",
    "        custom_objects={\n",
    "            'TokenAndPositionEmbedding': TokenAndPositionEmbedding,\n",
    "            'TransformerBlock': TransformerBlock\n",
    "        }\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# ------------------ Funci贸n de Predicci贸n ------------------\n",
    "def predecir_tramite(raw_input_sequence, tramits_csv, max_seq_len, model):\n",
    "    \"\"\"\n",
    "    Filtra tr谩mites por 'Vigent=True' y realiza la predicci贸n.\n",
    "    raw_input_sequence: lista de tr谩mites (secuencia de entrada).\n",
    "    tramits_csv: Ruta al archivo tramits.csv.\n",
    "    max_seq_len: Longitud m谩xima de la secuencia (padding).\n",
    "    model: Modelo cargado con cargar_modelo.\n",
    "    \"\"\"\n",
    "    tramits_df = pd.read_csv(tramits_csv)\n",
    "    vigent_indices = tramits_df[tramits_df['Vigent'] == True].index.tolist()\n",
    "\n",
    "    # Preparar el input\n",
    "    input_sequence = pad_sequences([raw_input_sequence], maxlen=max_seq_len, padding='post', truncating='post')\n",
    "\n",
    "    # Predicci贸n\n",
    "    predictions = model.predict(input_sequence)\n",
    "    filtered_predictions = {idx: predictions[0][idx] for idx in vigent_indices}\n",
    "    sorted_predictions = sorted(filtered_predictions.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Obtener las mejores predicciones\n",
    "    top_10_predictions = sorted_predictions[:5]\n",
    "    recommended_tramit, recommended_prob = top_10_predictions[0]\n",
    "\n",
    "    return recommended_tramit, recommended_prob, top_10_predictions\n",
    "\n",
    "def call_function(tramit_input):\n",
    "\n",
    "    # Cargar el modelo preentrenado\n",
    "    model = cargar_modelo(\"modelo_transformer_tramites.h5\")\n",
    "\n",
    "    # Realizar una predicci贸n\n",
    "    raw_input_sequence = tramit_input  # Entrada ejemplo\n",
    "    tramits_csv = \"../../data/tramits.csv\"  # Ruta al archivo tramits.csv\n",
    "    max_seq_len = 20  # Longitud m谩xima de secuencias\n",
    "\n",
    "    # Realizar la predicci贸n \n",
    "    recommended_tramit, _, top_10_predictions = predecir_tramite(\n",
    "        raw_input_sequence, tramits_csv, max_seq_len, model\n",
    "    )\n",
    "    \n",
    "    # Filtrar predicciones con probabilidad de al menos 2%\n",
    "    filtered_predictions = [pred for pred in top_10_predictions if pred[1] >= 0.0001]\n",
    "    print(filtered_predictions)\n",
    "\n",
    "    # Leer el archivo tramits.csv para obtener los t铆tulos\n",
    "    tramits_df = pd.read_csv(tramits_csv)\n",
    "    \n",
    "    # Obtener los t铆tulos correspondientes a los 铆ndices de las predicciones filtradas\n",
    "    top_10_titles = [tramits_df.loc[tramits_df['Sequence'] == idx, 'Titol'].values[0] for idx, _ in filtered_predictions]\n",
    "    \n",
    "    # Ver resultados\n",
    "    if filtered_predictions:\n",
    "        print(f\"Tr谩mite recomendado: {tramits_df.loc[tramits_df['Sequence'] == filtered_predictions[0][0], 'Titol'].values[0]}\")\n",
    "        print(\"\\nTop predicciones con al menos 2% de probabilidad:\")\n",
    "        for title in top_10_titles:\n",
    "            print(title)\n",
    "    else:\n",
    "        print(\"Tr谩mite recomendado: Sol路licitud gen猫rica\")\n",
    "    \n",
    "    return top_10_titles if filtered_predictions else [\"Sol路licitud gen猫rica\"]\n",
    "call_function([142, 145, 145, 145, 51, 307, 234, 51, 307, 234, 51, 307, 307, 307])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
