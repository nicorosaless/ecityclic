{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the original CSV file\n",
    "df = pd.read_csv('../../data/accions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Sessio' and aggregate the 'Accio' column into a list\n",
    "grouped_df = df.groupby('Sessio').agg({'Accio': list, 'Tramit': list, 'Usuari': list, 'Data': list}).reset_index()\n",
    "\n",
    "\n",
    "# Write the grouped data to a new CSV file\n",
    "grouped_df.to_csv('../../data/grouped_accions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df['Usuari'] = grouped_df['Usuari'].apply(lambda x: x[-1] if isinstance(x, list) and x else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the grouped data to a new CSV file\n",
    "grouped_df.to_csv('../../data/grouped_accions_usuari.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped_df = pd.read_csv('../../data/grouped_accions_usuari.csv')\n",
    "(grouped_df['Tramit'].apply(lambda x: len((x))) == 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = grouped_df[~((grouped_df['Usuari'].isna()) & (grouped_df['Tramit'].apply(lambda x: len((x))) == 1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the grouped data to a new CSV file\n",
    "filtered_df.to_csv('../../data/grouped_accions_filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = pd.read_csv('../../data/grouped_accions_filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = filtered_df[~(filtered_df['Tramit'].apply(lambda x: len(eval(x))) == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.drop(columns=['Usuari', 'Accio', 'Data', 'Sessio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('../../data/grouped_accions_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.read_csv('../../data/grouped_accions_final.csv')\n",
    "\n",
    "#### \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['Last_Tramit'] = final_df['Tramit'].apply(lambda x: eval(x)[-1] if isinstance(eval(x), list) and eval(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['Tramit'] = final_df['Tramit'].apply(lambda x: str(eval(x)[:-1]) if isinstance(eval(x), list) and eval(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('../../data/final_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final_df = pd.read_csv('../../data/final_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_final_df['Tramit']\n",
    "y_train = train_final_df['Last_Tramit']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un dataset de muestra con 50 filas\n",
    "sample_df = train_final_df.sample(n=50, random_state=42)\n",
    "\n",
    "# Guardar el dataset de muestra en el directorio\n",
    "sample_df.to_csv('../../data/sample_train_final_df.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the sample_tramits.csv file\n",
    "sample_tramits_df = pd.read_csv('../../data/sample_tramits.csv')\n",
    "\n",
    "# Erase the 'Vigent' column\n",
    "sample_tramits_df = sample_tramits_df.drop(columns=['Vigent'])\n",
    "\n",
    "# Display the first few rows of the dataframe to verify\n",
    "print(sample_tramits_df.head())\n",
    "\n",
    "sample_tramits_df.to_csv('../../data/sample_tramits_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tramits_df = pd.read_csv('../../data/tramits.csv')\n",
    "\n",
    "\n",
    "tramits_df['Sequence'] = range(tramits_df.shape[0])\n",
    "\n",
    "\n",
    "tramits_df.to_csv('../../data/tramits.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_sequence = dict(zip(tramits_df['Id'], tramits_df['Sequence']))\n",
    "\n",
    "final_df['Tramit'] = final_df['Tramit'].apply(lambda x: [id_to_sequence.get(tramit, tramit) for tramit in eval(x)])\n",
    "final_df['Last_Tramit'] = final_df['Last_Tramit'].apply(lambda x: id_to_sequence.get(x, x))\n",
    "\n",
    "final_df.to_csv('../../data/final_dataset_mapped.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_df = pd.read_csv('../../data/final_dataset_mapped.csv')\n",
    "final_df = final_df[final_df['Tramit'].apply(len) <= 30]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Distribución de y_train:\", pd.Series(y_train).value_counts())\n",
    "\n",
    "print(\"Distribución del 82 en y_train:\", (y_train == 314).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, LayerNormalization, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "# ------------------ Carga y Preprocesamiento del Dataset ------------------\n",
    "print(\"### Carga y Preprocesamiento del Dataset ###\")\n",
    "file_path = \"../../data/final_dataset_mapped.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Preprocesar las columnas\n",
    "data['Tramit'] = data['Tramit'].apply(lambda x: ast.literal_eval(x))  # Convertir strings en listas\n",
    "if data.isnull().values.any():\n",
    "    raise ValueError(\"El archivo CSV contiene valores nulos.\")\n",
    "data['Last_Tramit'] = data['Last_Tramit'].astype(int)\n",
    "\n",
    "# Preparar las features 'X' y target 'y'\n",
    "max_seq_len = 20\n",
    "X_raw = data['Tramit'].tolist()\n",
    "y_data = data['Last_Tramit'].values\n",
    "X_data = pad_sequences(X_raw, maxlen=max_seq_len, padding='post', truncating='post')  # Padding de secuencias\n",
    "\n",
    "# Dividir los datos en entrenamiento (80%) y prueba (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# ------------------ Definición de Capas Personalizadas ------------------\n",
    "print(\"### Definición de Capas Personalizadas ###\")\n",
    "class TokenAndPositionEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim, **kwargs):\n",
    "        super(TokenAndPositionEmbedding, self).__init__(**kwargs)\n",
    "        print(f\"Inicializando TokenAndPositionEmbedding con maxlen={maxlen}, vocab_size={vocab_size}, embed_dim={embed_dim}\")\n",
    "        self.maxlen = maxlen\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Capas Embedding\n",
    "        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        max_len_positions = tf.range(start=0, limit=self.maxlen, delta=1)\n",
    "        positions = self.pos_emb(max_len_positions)  # Generar embeddings posicionales\n",
    "        tokens = self.token_emb(x)  # Generar embeddings de tokens\n",
    "        return tokens + positions  # Combinar tokens y posiciones\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(TokenAndPositionEmbedding, self).get_config()\n",
    "        config.update({\n",
    "            \"maxlen\": self.maxlen,\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "            \"embed_dim\": self.embed_dim\n",
    "        })\n",
    "        print(f\"Serializando capa TokenAndPositionEmbedding con config: {config}\")\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        print(f\"Reconstruyendo capa TokenAndPositionEmbedding desde config: {config}\")\n",
    "        return cls(**config)\n",
    "\n",
    "\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        print(f\"Inicializando TransformerBlock con embed_dim={embed_dim}, num_heads={num_heads}, ff_dim={ff_dim}, rate={rate}\")\n",
    "\n",
    "        # Guardar configuraciones\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.rate = rate\n",
    "\n",
    "        # Creación de capas\n",
    "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation=\"relu\"),\n",
    "            Dense(embed_dim)\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(TransformerBlock, self).get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"ff_dim\": self.ff_dim,\n",
    "            \"rate\": self.rate\n",
    "        })\n",
    "        print(f\"Serializando capa TransformerBlock con config: {config}\")\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        print(f\"Reconstruyendo capa TransformerBlock desde config: {config}\")\n",
    "        return cls(**config)\n",
    "\n",
    "# Resto del código se mantiene igual.\n",
    "# El modelo utilizará ahora correctamente la clase TokenAndPositionEmbedding.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------ Definición del Modelo ------------------\n",
    "\n",
    "print(\"### Definición del Modelo ###\")\n",
    "\n",
    "def create_model(vocab_size, max_seq_len, embed_dim, num_heads, ff_dim, num_transformer_blocks, dropout_rate):\n",
    "    inputs = Input(shape=(max_seq_len,))\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen=max_seq_len, vocab_size=vocab_size, embed_dim=embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "\n",
    "    for i in range(num_transformer_blocks):\n",
    "        x = TransformerBlock(embed_dim, num_heads, ff_dim, rate=dropout_rate)(x)\n",
    "        print(f\"Creado TransformerBlock {i + 1} con Dropout Rate = {dropout_rate}\")\n",
    "\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    outputs = Dense(vocab_size, activation=\"softmax\")(x)\n",
    "    return Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "# ------------------ Ejecución Multi-Configuración ------------------\n",
    "print(\"### Ejecución Multi-Configuración ###\")\n",
    "configs = pd.DataFrame([  # ~20 min\n",
    "    # Modelos grandes:,# ~30 min\n",
    "    {\"embed_dim\": 512, \"num_heads\": 16, \"ff_dim\": 1024, \"num_transformer_blocks\": 4, \"epochs\": 8},# ~25-30 min\n",
    "    {\"embed_dim\": 256, \"num_heads\": 8, \"ff_dim\": 512, \"num_transformer_blocks\": 3, \"epochs\": 15}, # ~30+ min\n",
    "    {\"embed_dim\": 512, \"num_heads\": 16, \"ff_dim\": 1024, \"num_transformer_blocks\": 4, \"epochs\": 15},\n",
    "])\n",
    "\n",
    "if not os.path.exists('../../models/'):\n",
    "    os.makedirs('../../models/')\n",
    "\n",
    "results = []\n",
    "vocab_size = 502\n",
    "\n",
    "for index, config in configs.iterrows():\n",
    "    print(f\"\\n### Ejecutando Configuración {index + 1}/{len(configs)}: {config.to_dict()}\")\n",
    "    embed_dim = config['embed_dim']\n",
    "    num_heads = config['num_heads']\n",
    "    ff_dim = config['ff_dim']\n",
    "    num_transformer_blocks = config['num_transformer_blocks']\n",
    "    epochs = config['epochs']\n",
    "    dropout_rate = 0.2\n",
    "\n",
    "    # Crear el modelo\n",
    "    model = create_model(vocab_size, max_seq_len, embed_dim, num_heads, ff_dim, num_transformer_blocks, dropout_rate)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    start_time = time.time()\n",
    "    early_stop = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=32,\n",
    "        epochs=epochs,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stop],\n",
    "        verbose=1\n",
    "    )\n",
    "    training_time = time.time() - start_time\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "    # Guardar el modelo\n",
    "    model_save_path = f\"../../models/model_config_{index + 1}.h5\"\n",
    "    try:\n",
    "        print(f\"Guardando modelo en: {model_save_path}\")\n",
    "        model.save(model_save_path)\n",
    "        print(f\"Modelo guardado con éxito en {model_save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al guardar el modelo: {e}\")\n",
    "        continue  # Saltar esta configuración si falla\n",
    "\n",
    "    # Guardar resultados\n",
    "    results.append({\n",
    "        \"config_index\": index,\n",
    "        \"embed_dim\": embed_dim,\n",
    "        \"num_heads\": num_heads,\n",
    "        \"ff_dim\": ff_dim,\n",
    "        \"num_transformer_blocks\": num_transformer_blocks,\n",
    "        \"epochs\": epochs,\n",
    "        \"train_accuracy\": history.history['accuracy'][-1],\n",
    "        \"val_accuracy\": max(history.history['val_accuracy']),\n",
    "        \"test_accuracy\": test_accuracy,\n",
    "        \"test_loss\": test_loss,\n",
    "        \"training_time\": training_time,\n",
    "        \"model_path\": model_save_path\n",
    "    })\n",
    "\n",
    "# Consolidar resultados\n",
    "if len(results) > 0:  # Comprobar que haya resultados\n",
    "    try:\n",
    "        # Convertir lista de diccionarios en DataFrame\n",
    "        results_df = pd.DataFrame(results)\n",
    "\n",
    "        # Verificar que el DataFrame no esté vacío\n",
    "        if results_df.empty:\n",
    "            print(\"*** El DataFrame generado a partir de 'results' está vacío. No se guardará ningún archivo.\")\n",
    "        else:\n",
    "            # Guardar resultados en un archivo CSV\n",
    "            results_csv_path = '../../data/results.csv'\n",
    "\n",
    "            # Crear el directorio si no existe\n",
    "            output_dir = os.path.dirname(results_csv_path)\n",
    "            if not os.path.exists(output_dir):  # Verificar existencia de la carpeta\n",
    "                os.makedirs(output_dir)\n",
    "                print(f\"### Directorio creado: {output_dir}\")\n",
    "\n",
    "            # Guardar DataFrame en archivo CSV\n",
    "            results_df.to_csv(results_csv_path, index=False)\n",
    "            print(f\"### Resultados guardados en el archivo: {results_csv_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"*** Error al guardar los resultados: {str(e)}\")\n",
    "else:\n",
    "    print(\"*** No hay resultados para guardar. La lista 'results' está vacía.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pruebas de predicciones\n",
    "\n",
    "# ------------------ Capas Personalizadas ------------------\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([tf.keras.layers.Dense(ff_dim, activation=\"relu\"), tf.keras.layers.Dense(embed_dim)])\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.dropout1(self.att(inputs, inputs, training=training), training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.dropout2(self.ffn(out1, training=training), training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "\n",
    "class TokenAndPositionEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = tf.keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        positions = self.pos_emb(tf.range(start=0, limit=tf.shape(x)[-1]))\n",
    "        return self.token_emb(x) + positions\n",
    "\n",
    "\n",
    "# ------------------ Modelo Preentrenado ------------------\n",
    "def cargar_modelo(ruta_modelo=\"modelo_transformer_tramites.h5\"):\n",
    "    \"\"\"Carga el modelo Transformer preentrenado.\"\"\"\n",
    "    model = tf.keras.models.load_model(\n",
    "        ruta_modelo,\n",
    "        custom_objects={\n",
    "            'TokenAndPositionEmbedding': TokenAndPositionEmbedding,\n",
    "            'TransformerBlock': TransformerBlock\n",
    "        }\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# ------------------ Función de Predicción ------------------\n",
    "def predecir_tramite(raw_input_sequence, tramits_csv, max_seq_len, model):\n",
    "    \"\"\"\n",
    "    Filtra trámites por 'Vigent=True' y realiza la predicción.\n",
    "    raw_input_sequence: lista de trámites (secuencia de entrada).\n",
    "    tramits_csv: Ruta al archivo tramits.csv.\n",
    "    max_seq_len: Longitud máxima de la secuencia (padding).\n",
    "    model: Modelo cargado con cargar_modelo.\n",
    "    \"\"\"\n",
    "    tramits_df = pd.read_csv(tramits_csv)\n",
    "    vigent_indices = tramits_df[tramits_df['Vigent'] == True].index.tolist()\n",
    "\n",
    "    # Preparar el input\n",
    "    input_sequence = pad_sequences([raw_input_sequence], maxlen=max_seq_len, padding='post', truncating='post')\n",
    "\n",
    "    # Predicción\n",
    "    predictions = model.predict(input_sequence)\n",
    "    filtered_predictions = {idx: predictions[0][idx] for idx in vigent_indices}\n",
    "    sorted_predictions = sorted(filtered_predictions.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Obtener las mejores predicciones\n",
    "    top_10_predictions = sorted_predictions[:5]\n",
    "    recommended_tramit, recommended_prob = top_10_predictions[0]\n",
    "\n",
    "    return recommended_tramit, recommended_prob, top_10_predictions\n",
    "\n",
    "def call_function(tramit_input):\n",
    "\n",
    "    # Cargar el modelo preentrenado\n",
    "    model = cargar_modelo(\"modelo_transformer_tramites.h5\")\n",
    "\n",
    "    # Realizar una predicción\n",
    "    raw_input_sequence = tramit_input  # Entrada ejemplo\n",
    "    tramits_csv = \"../../data/tramits.csv\"  # Ruta al archivo tramits.csv\n",
    "    max_seq_len = 20  # Longitud máxima de secuencias\n",
    "\n",
    "    # Realizar la predicción \n",
    "    recommended_tramit, _, top_10_predictions = predecir_tramite(\n",
    "        raw_input_sequence, tramits_csv, max_seq_len, model\n",
    "    )\n",
    "    \n",
    "    # Filtrar predicciones con probabilidad de al menos 2%\n",
    "    filtered_predictions = [pred for pred in top_10_predictions if pred[1] >= 0.0001]\n",
    "    print(filtered_predictions)\n",
    "\n",
    "    # Leer el archivo tramits.csv para obtener los títulos\n",
    "    tramits_df = pd.read_csv(tramits_csv)\n",
    "    \n",
    "    # Obtener los títulos correspondientes a los índices de las predicciones filtradas\n",
    "    top_10_titles = [tramits_df.loc[tramits_df['Sequence'] == idx, 'Titol'].values[0] for idx, _ in filtered_predictions]\n",
    "    \n",
    "    # Ver resultados\n",
    "    if filtered_predictions:\n",
    "        print(f\"Trámite recomendado: {tramits_df.loc[tramits_df['Sequence'] == filtered_predictions[0][0], 'Titol'].values[0]}\")\n",
    "        print(\"\\nTop predicciones con al menos 2% de probabilidad:\")\n",
    "        for title in top_10_titles:\n",
    "            print(title)\n",
    "    else:\n",
    "        print(\"Trámite recomendado: Sol·licitud genèrica\")\n",
    "    \n",
    "    return top_10_titles if filtered_predictions else [\"Sol·licitud genèrica\"]\n",
    "call_function([142, 145, 145, 145, 51, 307, 234, 51, 307, 234, 51, 307, 307, 307])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
